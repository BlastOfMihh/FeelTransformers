{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvuPzkVtDlA9"
      },
      "source": [
        "#1: Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob5Yn9gWCzZo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_parquet(\"hf://datasets/ucirvine/sms_spam/plain_text/train-00000-of-00001.parquet\")\n",
        "\n",
        "# Rename columns for clarity\n",
        "df.columns = ['sms', 'label']\n",
        "\n",
        "# Check for missing values\n",
        "df.dropna(subset=['sms', 'label'], inplace=True)\n",
        "\n",
        "# Map the labels: 1 for 'spam', 0 for 'ham'\n",
        "df['label'] = df['label'].apply(lambda x: 1 if x == 'spam' else 0)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['sms'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the tokenizer (we'll use BERT tokenizer)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def tokenize_and_pad(texts, tokenizer, max_length=64):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=True,      # Pad to max length\n",
        "        truncation=True,   # Truncate longer than max_length\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_and_pad(train_texts, tokenizer)\n",
        "val_encodings = tokenize_and_pad(val_texts, tokenizer)\n",
        "\n",
        "# Create a custom Dataset class\n",
        "class SMSDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create data loaders for training and validation\n",
        "train_dataset = SMSDataset(train_encodings, train_labels)\n",
        "val_dataset = SMSDataset(val_encodings, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frj9LwU_Ey5M"
      },
      "source": [
        "# 2 Regular Transformer architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHd9VqF8E4pb"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, hidden_size=768, num_attention_blocks=2, num_classes=2):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Transformer Encoder Blocks (2 blocks in total)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=hidden_size,\n",
        "                nhead=12,  # Number of attention heads\n",
        "                dim_feedforward=hidden_size*4\n",
        "            ) for _ in range(num_attention_blocks)\n",
        "        ])\n",
        "\n",
        "        # Output layer for classification\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # BERT embedding outputs: [batch_size, seq_len, hidden_size]\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        # Pass through the transformer blocks (additional layers on top of BERT)\n",
        "        for block in self.transformer_blocks:\n",
        "            hidden_states = block(hidden_states)\n",
        "\n",
        "        # Use the hidden state of the [CLS] token for classification (first token)\n",
        "        cls_output = hidden_states[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "        # Classify the [CLS] token output\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleTransformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLfdMCipGbZx"
      },
      "source": [
        "# 3 Training Simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAKLweJUMGLH",
        "outputId": "89a28ff4-8f60-4e29-a035-cda386c33554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PaS1VYAGd8y",
        "outputId": "374232cb-e16f-4886-d462-f44732adf3b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-bcecb61f26aa>:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "Training loss: 0.0089, Training accuracy: 0.9930\n",
            "Validation accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Set device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_preds += (preds == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(\"batch done\")\n",
        "\n",
        "    return total_loss / len(train_loader), correct_preds / total_preds\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_preds += (preds == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "    return correct_preds / total_preds\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, loss_fn)\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Training loss: {train_loss:.4f}, Training accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Validation accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZdfI3UDgvzy"
      },
      "source": [
        "# 4 Custom stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mifVfvYaScfT",
        "outputId": "6c488ea5-7eab-4ef0-f633-09ff78090864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input text: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "Predicted label: 0\n"
          ]
        }
      ],
      "source": [
        "# prompt: test the model on a single custom example\n",
        "\n",
        "import torch\n",
        "\n",
        "# Assuming 'model' and 'tokenizer' are defined from the previous code\n",
        "\n",
        "def predict_single_example(text, model, tokenizer):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=64, return_tensors=\"pt\")\n",
        "\n",
        "    # Move inputs to the device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Make the prediction\n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Get the predicted class (0 or 1)\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "# Example usage\n",
        "custom_text = \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"  # Example spam text\n",
        "predicted_label = predict_single_example(custom_text, model, tokenizer)\n",
        "\n",
        "print(f\"Input text: {custom_text}\")\n",
        "print(f\"Predicted label: {predicted_label}\") # 1 for spam, 0 for ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMMbGphiFoTt"
      },
      "source": [
        "# 2 Feelings-Based Transformer architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUVycMlZFvsw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class FeelTransformer(nn.Module):\n",
        "    def __init__(self, hidden_size=768, num_attention_blocks=4, num_classes=2):\n",
        "        super(FeelTransformer, self).__init__()\n",
        "\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Transformer Encoder Blocks (2 blocks in total)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            FeelEncoderLayer(\n",
        "                hidden_size=hidden_size,\n",
        "                num_heads=12,  # Number of attention heads\n",
        "                dim_feedforward=hidden_size*4\n",
        "            ) for _ in range(num_attention_blocks)\n",
        "        ])\n",
        "\n",
        "        # Output layer for classification\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # BERT embedding outputs: [batch_size, seq_len, hidden_size]\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        # Pass through the transformer blocks (additional layers on top of BERT)\n",
        "        for block in self.transformer_blocks:\n",
        "            hidden_states = block(hidden_states)\n",
        "\n",
        "        # Use the hidden state of the [CLS] token for classification (first token)\n",
        "        cls_output = hidden_states[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "        # Classify the [CLS] token output\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model\n",
        "model = FeelTransformer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqtBddX_cVvk"
      },
      "outputs": [],
      "source": [
        "class FeelEncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_size=768, num_heads=12,dim_feedforward=768*4, dropout=0.1, activation_function=nn.ReLU()):\n",
        "        super(FeelEncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-Head Attention Layer\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, dropout=dropout)\n",
        "        self.self_emotion = EmotionLayer(embed_dim=hidden_size, emotion_dim=hidden_size)\n",
        "\n",
        "\n",
        "       #mihh stuff\n",
        "      #  self.feeling_matrix = nn.Parameter( torch.randn((3, 4)) )\n",
        "\n",
        "\n",
        "        # Layer normalization before and after attention\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # Feedforward network (with custom activation)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(hidden_size, dim_feedforward),  # First linear layer\n",
        "            activation_function,                      # Custom activation\n",
        "            nn.Linear(dim_feedforward, hidden_size)   # Second linear layer\n",
        "        )\n",
        "\n",
        "        # Layer normalization before and after the feed-forward network\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(dropout)  # Dropout after attention\n",
        "        self.dropout2 = nn.Dropout(dropout)  # Dropout after feed-forward\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: [seq_len, batch_size, hidden_size]\n",
        "        # mask: [batch_size, seq_len] (optional for padding)\n",
        "\n",
        "        # Multi-Head Self-Attention with residual connection and layer norm\n",
        "        attn_output, _ = self.self_attention(x, x, x, attn_mask=mask)\n",
        "\n",
        "        #x=self.self_emotion(x, query_feel=None)\n",
        "\n",
        "        x = x + self.dropout1(attn_output)  # Add residual connection\n",
        "        x = self.norm1(x)  # Apply layer normalization\n",
        "\n",
        "\n",
        "\n",
        "        # Feedforward layer with residual connection and layer norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout2(ff_output)  # Add residual connection\n",
        "        x = self.norm2(x)  # Apply layer normalization\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNE2QB5xjsnd"
      },
      "outputs": [],
      "source": [
        "#the math\n",
        "\n",
        "# delta_feel = X * EmotionSpace * downScale\n",
        "# feel += delta_feel\n",
        "#\n",
        "\n",
        "# feel_pdf = softmax ( query_feel * X )\n",
        "# X += feel_pdf * feel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZQiEB313Ptz"
      },
      "outputs": [],
      "source": [
        "class EmotionLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, emotion_dim, downscale=1.0):\n",
        "        \"\"\"\n",
        "        Initialize the EmotionLayer.\n",
        "        :param embed_dim: Dimensionality of the input X (same as embed_dim of attention layers).\n",
        "        :param emotion_dim: Dimensionality of the EmotionSpace.\n",
        "        :param downscale: Scaling factor for delta_feel calculation.\n",
        "        \"\"\"\n",
        "        super(EmotionLayer, self).__init__()\n",
        "        self.emotion_space = nn.Parameter(torch.randn(emotion_dim, embed_dim))\n",
        "        self.downscale = downscale\n",
        "        self.feel = nn.Parameter(torch.zeros(emotion_dim))\n",
        "\n",
        "    def forward(self, X, query_feel):\n",
        "        \"\"\"\n",
        "        Forward pass of the EmotionLayer.\n",
        "        :param X: Input tensor of shape (seq_len, batch_size, embed_dim).\n",
        "        :param query_feel: Query tensor for calculating feel_pdf, shape (batch_size, emotion_dim).\n",
        "        :return: Updated X tensor of shape (seq_len, batch_size, embed_dim).\n",
        "        \"\"\"\n",
        "        # Transpose X to (batch_size, seq_len, embed_dim) for processing\n",
        "        X = X.transpose(0, 1)\n",
        "\n",
        "        # Calculate delta_feel\n",
        "        delta_feel = torch.einsum('bsi,ij->bsj', X, self.emotion_space)  # (batch_size, seq_len, emotion_dim)\n",
        "        delta_feel = torch.mean(delta_feel, dim=1) * self.downscale  # (batch_size, emotion_dim)\n",
        "        self.feel = self.feel + delta_feel  # Update feel\n",
        "\n",
        "        # Calculate feel_pdf using softmax\n",
        "        feel_pdf = F.softmax(torch.einsum('bi,ij->bj', query_feel, self.emotion_space.T), dim=-1)  # (batch_size, embed_dim)\n",
        "\n",
        "        # Update X\n",
        "        X_updated = X + torch.einsum('bi,bj->bij', feel_pdf, self.feel)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # Transpose X back to (seq_len, batch_size, embed_dim)\n",
        "        return X_updated.transpose(0, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 Comparing the FeelTransformer with the regular Tranformer"
      ],
      "metadata": {
        "id": "_41BBh3MxEjR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fys37XxowlI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# make this single Feeling head\n",
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout=0.1):\n",
        "        super(SingleHeadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.query_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value, attention_mask=None):\n",
        "        # Compute query, key, and value vectors\n",
        "        query = self.query_linear(query)\n",
        "        key = self.key_linear(key)\n",
        "        value = self.value_linear(value)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.embed_dim)\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Apply dropout to attention weights\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Compute output\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M5_YdZ_kiR9F"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}